{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Crawler\n",
    "Using praw to comb through reddit posts, then PIL to process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import praw\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configurations for the crawler and image downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS_LIST_PATH = \"./subreddit_list.txt\"\n",
    "MAX_IMG = 500\n",
    "ACCEPTABLE_EXTENSIONS = [\"jpg\", \"png\"]\n",
    "MAX_RESOLUTION = (1024, 1024)\n",
    "\n",
    "# Note: you will need your own praw.ini config file to use this command\n",
    "reddit = praw.Reddit(\"cs4243\")\n",
    "req_header = { \"User-Agent\": \"CS4243 crawler bot\", \"From\": \"insert email here\" }\n",
    "with open(SUBREDDITS_LIST_PATH, \"r\") as f:\n",
    "    sr_list = [ x.strip() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get image metadata first before deciding on images to download. Due to the long tailed distribution, and for a more representative distribution of scores, we download 500 images per subreddit so that the calculated percentiles are representative, and that there are enough images in the popular class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: THIS CODE SEGMENT CAN TAKE MORE THAN TEN MINUTES TO RUN!\n",
    "now = datetime.datetime.now()\n",
    "unixnow = int(datetime.datetime.timestamp(now))\n",
    "PRELIMINARY_PATH = f\"./crawl_{now.month:02}{now.day:02}.csv\"\n",
    "\n",
    "with open(PRELIMINARY_PATH, 'w') as datafile:\n",
    "    datafile.write(\"ID,SCORE,SUBREDDIT,URL,UNIX TIME,UPVOTE RATIO\\n\")\n",
    "    for sr in sr_list:\n",
    "        count = 0\n",
    "        for submission in reddit.subreddit(sr).new(limit=None):\n",
    "            # posts are at least one week old, for score stability\n",
    "            if (unixnow - submission.created_utc) > 604800:  \n",
    "                srname = submission.subreddit.display_name.lower()\n",
    "                if submission.url[-3:] not in ACCEPTABLE_EXTENSIONS:\n",
    "                    continue\n",
    "                datafile.write(f\"{submission.id},{submission.score},{srname},{submission.url},\" + \\\n",
    "                                    f\"{submission.created_utc},{submission.upvote_ratio}\\n\")\n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    datafile.flush()\n",
    "                    print(f\"Sourcing images in {sr}: {count}/{MAX_IMG}\")\n",
    "                if count == MAX_IMG:\n",
    "                    datafile.flush()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing to download selected images based on class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for selecting images to download based on metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import queue\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "PERCENTILE_BINS = [0.5, 0.9, 1.0]\n",
    "NUM_ROWS_PER_SUB_PER_PERCENT = 50\n",
    "CSV_PATH = \"./data/reddit/processed_data.csv\"\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "with open(PRELIMINARY_PATH, \"r\") as f:\n",
    "    data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort reddit post scores into classes based on percentile bins. After which, select an equal number of images per subreddit per bin such that the proportion of classes are roughly equal. This solves the problem of learning on long-tailed distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PERCENTILE'] = data['SCORE'].rank(pct=True)\n",
    "data['PERCENTILE BIN'] = np.digitize(\n",
    "    data['PERCENTILE'], PERCENTILE_BINS, right=True)\n",
    "data['PERCENTILE BIN'] = data['PERCENTILE BIN'].map(\n",
    "    {index: bin for index, bin in enumerate(PERCENTILE_BINS)})\n",
    "\n",
    "filtered_rows = []\n",
    "for sr in sr_list:\n",
    "    os.makedirs(f\"./data/reddit/{sr.lower()}\", exist_ok=True)\n",
    "    for percent in PERCENTILE_BINS:\n",
    "        subdata = data[(data[\"SUBREDDIT\"] == sr.lower()) & (data[\"PERCENTILE BIN\"] == percent)]\n",
    "        filtered_rows.extend(subdata.head(NUM_ROWS_PER_SUB_PER_PERCENT).values.tolist())\n",
    "output = pd.DataFrame(filtered_rows, columns=data.columns)\n",
    "with open(CSV_PATH, \"w\") as f:\n",
    "    output.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some threading tools to help make downloading images faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_pool = queue.Queue()\n",
    "def get_image(url, filename) :\n",
    "    req = requests.get(url, stream=True, headers=req_header)\n",
    "    if not req.ok:\n",
    "        return\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in req.iter_content(1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    with Image.open(filename) as im:\n",
    "        im.thumbnail(MAX_RESOLUTION)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(filename[:-3]+\"jpeg\", \"JPEG\", quality=50, optimize=True)\n",
    "    os.remove(filename)\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        try:\n",
    "            subr, id, url = job_pool.get(timeout=60)\n",
    "            get_image(url, f\"./data/reddit/{subr.lower()}/{id}.{url[-3:]}\")\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        job_pool.task_done()\n",
    "\n",
    "all_threads = [threading.Thread(target=worker) for _ in range(MAX_WORKERS)]\n",
    "for t in all_threads:\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed with the downloading of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: THIS CODE SEGMENT CAN TAKE MORE THAN TEN MINUTES TO RUN!\n",
    "for srname, id, url in zip(data[\"SUBREDDIT\"], data[\"ID\"], data[\"URL\"]):\n",
    "    print(f\"{srname} {id} {url}\")\n",
    "    job_pool.put((srname, id, url))\n",
    "job_pool.join()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5df0192bec759d1f7484f54f673824e28388fbedcfd4d53bba5ff9ed1739130e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('reddit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
