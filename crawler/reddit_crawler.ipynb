{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Crawler\n",
    "Using praw to comb through reddit posts, then PIL to process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import praw\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configurations for the crawler and image downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS_LIST_PATH = \"./subreddit_list.txt\"\n",
    "MAX_IMG = 10\n",
    "ACCEPTABLE_EXTENSIONS = [\"jpg\", \"png\"]\n",
    "MAX_RESOLUTION = (1024, 1024)\n",
    "\n",
    "# Note: you will need your own praw.ini config file to use this command\n",
    "reddit = praw.Reddit(\"cs4243\")\n",
    "req_header = { \"User-Agent\": \"CS4243 crawler bot\", \"From\": \"insert email here\" }\n",
    "with open(SUBREDDITS_LIST_PATH, \"r\") as f:\n",
    "    sr_list = [ x.strip() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get image metadata first before deciding on images to download. Due to the long tailed distribution, and for a more representative distribution of scores, we download 500 images per subreddit so that the calculated percentiles are representative, and that there are enough images in the popular class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sourcing images in ImaginaryArchers: 10/10\n",
      "Sourcing images in ImaginaryAssassins: 10/10\n",
      "Sourcing images in ImaginaryAstronauts: 10/10\n",
      "Sourcing images in ImaginaryKnights: 10/10\n",
      "Sourcing images in ImaginaryLovers: 10/10\n",
      "Sourcing images in ImaginaryMythology: 10/10\n",
      "Sourcing images in ImaginaryNobles: 10/10\n",
      "Sourcing images in ImaginaryScholars: 10/10\n",
      "Sourcing images in ImaginarySoldiers: 10/10\n",
      "Sourcing images in ImaginaryWarriors: 10/10\n",
      "Sourcing images in ImaginaryWitches: 10/10\n",
      "Sourcing images in ImaginaryWizards: 10/10\n",
      "Sourcing images in ImaginaryAngels: 10/10\n",
      "Sourcing images in ImaginaryDwarves: 10/10\n",
      "Sourcing images in ImaginaryElves: 10/10\n",
      "Sourcing images in ImaginaryFaeries: 10/10\n",
      "Sourcing images in ImaginaryHumans: 10/10\n",
      "Sourcing images in ImaginaryImmortals: 10/10\n",
      "Sourcing images in ImaginaryMerfolk: 10/10\n",
      "Sourcing images in ImaginaryOrcs: 10/10\n",
      "Sourcing images in ImaginaryBattlefields: 10/10\n",
      "Sourcing images in ImaginaryCityscapes: 10/10\n",
      "Sourcing images in ImaginaryHellscapes: 10/10\n",
      "Sourcing images in ImaginaryMindscapes: 10/10\n",
      "Sourcing images in ImaginaryPathways: 10/10\n",
      "Sourcing images in ImaginarySeascapes: 10/10\n",
      "Sourcing images in ImaginarySkyscapes: 10/10\n",
      "Sourcing images in ImaginaryStarscapes: 10/10\n",
      "Sourcing images in ImaginaryWastelands: 10/10\n",
      "Sourcing images in ImaginaryWeather: 10/10\n",
      "Sourcing images in ImaginaryWildlands: 10/10\n",
      "Sourcing images in ImaginaryWorlds: 10/10\n",
      "Sourcing images in ImaginaryArchitecture: 10/10\n",
      "Sourcing images in ImaginaryCastles: 10/10\n",
      "Sourcing images in ImaginaryDwellings: 10/10\n",
      "Sourcing images in ImaginaryInteriors: 10/10\n",
      "Sourcing images in ImaginaryBeasts: 10/10\n",
      "Sourcing images in ImaginaryBehemoths: 10/10\n",
      "Sourcing images in ImaginaryDemons: 10/10\n",
      "Sourcing images in ImaginaryDragons: 10/10\n",
      "Sourcing images in ImaginaryElementals: 10/10\n",
      "Sourcing images in ImaginaryHorrors: 10/10\n",
      "Sourcing images in ImaginaryHybrids: 10/10\n",
      "Sourcing images in ImaginaryLeviathans: 10/10\n",
      "Sourcing images in ImaginaryMonsterGirls: 10/10\n",
      "Sourcing images in ImaginaryUndead: 10/10\n",
      "Sourcing images in ImaginaryWorldEaters: 10/10\n",
      "Sourcing images in ImaginaryArmor: 10/10\n",
      "Sourcing images in ImaginaryCybernetics: 10/10\n",
      "Sourcing images in ImaginaryCyberpunk: 10/10\n",
      "Sourcing images in ImaginaryFutureWar: 10/10\n",
      "Sourcing images in ImaginaryFuturism: 10/10\n",
      "Sourcing images in ImaginaryMechs: 10/10\n",
      "Sourcing images in ImaginaryRobotics: 10/10\n",
      "Sourcing images in ImaginaryStarships: 10/10\n",
      "Sourcing images in ImaginarySteampunk: 10/10\n",
      "Sourcing images in ImaginaryWeaponry: 10/10\n",
      "Sourcing images in AutumnPorn: 10/10\n",
      "Sourcing images in BeachPorn: 10/10\n",
      "Sourcing images in BotanicalPorn: 10/10\n",
      "Sourcing images in DesertPorn: 10/10\n",
      "Sourcing images in DestructionPorn: 10/10\n",
      "Sourcing images in EarthPorn: 10/10\n",
      "Sourcing images in FirePorn: 10/10\n",
      "Sourcing images in GeologyPorn: 10/10\n",
      "Sourcing images in lakeporn: 10/10\n",
      "Sourcing images in MushroomPorn: 10/10\n",
      "Sourcing images in seaporn: 10/10\n",
      "Sourcing images in SkyPorn: 10/10\n",
      "Sourcing images in spaceporn: 10/10\n",
      "Sourcing images in waterporn: 10/10\n",
      "Sourcing images in WeatherPorn: 10/10\n",
      "Sourcing images in winterporn: 10/10\n",
      "Sourcing images in AbandonedPorn: 10/10\n",
      "Sourcing images in AerialPorn: 10/10\n",
      "Sourcing images in ArchitecturePorn: 10/10\n",
      "Sourcing images in boatporn: 10/10\n",
      "Sourcing images in bridgeporn: 10/10\n",
      "Sourcing images in CabinPorn: 10/10\n",
      "Sourcing images in carporn: 10/10\n",
      "Sourcing images in CemeteryPorn: 10/10\n",
      "Sourcing images in CityPorn: 10/10\n",
      "Sourcing images in drydockporn: 10/10\n",
      "Sourcing images in F1Porn: 10/10\n",
      "Sourcing images in GunPorn: 10/10\n",
      "Sourcing images in Houseporn: 10/10\n",
      "Sourcing images in InfrastructurePorn: 10/10\n",
      "Sourcing images in Knifeporn: 10/10\n",
      "Sourcing images in MachinePorn: 10/10\n",
      "Sourcing images in MilitaryPorn: 10/10\n",
      "Sourcing images in policeporn: 10/10\n",
      "Sourcing images in retailporn: 10/10\n",
      "Sourcing images in RoadPorn: 10/10\n",
      "Sourcing images in ruralporn: 10/10\n",
      "Sourcing images in StarshipPorn: 10/10\n",
      "Sourcing images in steamporn: 10/10\n",
      "Sourcing images in ThingsCutInHalfPorn: 10/10\n",
      "Sourcing images in toolporn: 10/10\n",
      "Sourcing images in VillagePorn: 10/10\n",
      "Sourcing images in AdrenalinePorn: 10/10\n",
      "Sourcing images in AgriculturePorn: 10/10\n",
      "Sourcing images in AnimalPorn: 10/10\n",
      "Sourcing images in ClimbingPorn: 10/10\n",
      "Sourcing images in CulinaryPorn: 10/10\n",
      "Sourcing images in DessertPorn: 10/10\n",
      "Sourcing images in HumanPorn: 10/10\n",
      "Sourcing images in MegalithPorn: 10/10\n",
      "Sourcing images in TeaPorn: 10/10\n",
      "Sourcing images in AdPorn: 10/10\n",
      "Sourcing images in AlbumArtPorn: 10/10\n",
      "Sourcing images in ApocalypsePorn: 10/10\n",
      "Sourcing images in ArtPorn: 10/10\n",
      "Sourcing images in avporn: 10/10\n",
      "Sourcing images in DesignPorn: 10/10\n",
      "Sourcing images in ExposurePorn: 10/10\n",
      "Sourcing images in fashionporn: 10/10\n",
      "Sourcing images in FractalPorn: 10/10\n",
      "Sourcing images in GamerPorn: 10/10\n",
      "Sourcing images in GeekPorn: 10/10\n",
      "Sourcing images in InstrumentPorn: 10/10\n",
      "Sourcing images in MacroPorn: 10/10\n",
      "Sourcing images in MetalPorn: 10/10\n",
      "Sourcing images in MoviePosterPorn: 10/10\n",
      "Sourcing images in OrganizationPorn: 10/10\n",
      "Sourcing images in RoomPorn: 10/10\n",
      "Sourcing images in SculpturePorn: 10/10\n",
      "Sourcing images in StreetArtPorn: 10/10\n",
      "Sourcing images in uniformporn: 10/10\n",
      "Sourcing images in ViewPorn: 10/10\n",
      "Sourcing images in ArtefactPorn: 10/10\n",
      "Sourcing images in bookporn: 10/10\n",
      "Sourcing images in FossilPorn: 10/10\n",
      "Sourcing images in futureporn: 10/10\n",
      "Sourcing images in HistoryPorn: 10/10\n"
     ]
    }
   ],
   "source": [
    "# CAUTION: THIS CODE SEGMENT CAN TAKE MORE THAN TEN MINUTES TO RUN!\n",
    "now = datetime.datetime.now()\n",
    "unixnow = int(datetime.datetime.timestamp(now))\n",
    "PRELIMINARY_PATH = f\"crawl_{now.month:02}{now.day:02}.csv\"\n",
    "\n",
    "with open(PRELIMINARY_PATH, 'w') as datafile:\n",
    "    datafile.write(\"ID,SCORE,SUBREDDIT,URL,UNIX TIME,UPVOTE RATIO\\n\")\n",
    "    for sr in sr_list:\n",
    "        count = 0\n",
    "        for submission in reddit.subreddit(sr).new(limit=None):\n",
    "            # posts are at least one week old, for score stability\n",
    "            if (unixnow - submission.created_utc) > 604800:  \n",
    "                srname = submission.subreddit.display_name.lower()\n",
    "                if submission.url[-3:] not in ACCEPTABLE_EXTENSIONS:\n",
    "                    continue\n",
    "                datafile.write(f\"{submission.id},{submission.score},{srname},{submission.url},\" + \\\n",
    "                                    f\"{submission.created_utc},{submission.upvote_ratio}\\n\")\n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    datafile.flush()\n",
    "                    print(f\"Sourcing images in {sr}: {count}/{MAX_IMG}\")\n",
    "                if count == MAX_IMG:\n",
    "                    datafile.flush()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing to download selected images based on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import queue\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "PERCENTILE_BINS = [0.5, 0.9, 1.0]\n",
    "NUM_ROWS_PER_SUB_PER_PERCENT = 10\n",
    "CSV_PATH = \"../data/reddit/processed_data.csv\"\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "with open(PRELIMINARY_PATH, \"r\") as f:\n",
    "    data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PERCENTILE'] = data['SCORE'].rank(pct=True)\n",
    "data['PERCENTILE BIN'] = np.digitize(\n",
    "    data['PERCENTILE'], PERCENTILE_BINS, right=True)\n",
    "data['PERCENTILE BIN'] = data['PERCENTILE BIN'].map(\n",
    "    {index: bin for index, bin in enumerate(PERCENTILE_BINS)})\n",
    "\n",
    "filtered_rows = []\n",
    "for sr in sr_list:\n",
    "    os.makedirs(f\"../data/reddit/{sr.lower()}\", exist_ok=True)\n",
    "    for percent in PERCENTILE_BINS:\n",
    "        subdata = data[(data[\"SUBREDDIT\"] == sr.lower()) & (data[\"PERCENTILE BIN\"] == percent)]\n",
    "        filtered_rows.extend(subdata.head(NUM_ROWS_PER_SUB_PER_PERCENT).values.tolist())\n",
    "output = pd.DataFrame(filtered_rows, columns=data.columns)\n",
    "with open(CSV_PATH, \"w\") as f:\n",
    "    output.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_pool = queue.Queue()\n",
    "def get_image(url, filename) :\n",
    "    req = requests.get(url, stream=True, headers=req_header)\n",
    "    if not req.ok:\n",
    "        return\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in req.iter_content(1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    with Image.open(filename) as im:\n",
    "        im.thumbnail(MAX_RESOLUTION)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(filename[:-3]+\"jpeg\", \"JPEG\", quality=50, optimize=True)\n",
    "    os.remove(filename)\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        try:\n",
    "            subr, id, url = job_pool.get(timeout=60)\n",
    "            get_image(url, f\"../data/reddit/{subr.lower()}/{id}.{url[-3:]}\")\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        job_pool.task_done()\n",
    "\n",
    "all_threads = [threading.Thread(target=worker) for _ in range(MAX_WORKERS)]\n",
    "for t in all_threads:\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for srname, id, url in zip(data[\"SUBREDDIT\"], data[\"ID\"], data[\"URL\"]):\n",
    "    print(f\"{srname} {id} {url}\")\n",
    "    job_pool.put((srname, id, url))\n",
    "job_pool.join()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5df0192bec759d1f7484f54f673824e28388fbedcfd4d53bba5ff9ed1739130e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('reddit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
