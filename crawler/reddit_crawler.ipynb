{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Crawler\n",
    "Using praw to comb through reddit posts, then PIL to process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import praw\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configurations for the crawler and image downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS_LIST_PATH = \"./subreddit_list.txt\"\n",
    "MAX_IMG = 500\n",
    "ACCEPTABLE_EXTENSIONS = [\"jpg\", \"png\"]\n",
    "MAX_RESOLUTION = (1024, 1024)\n",
    "\n",
    "# Note: you will need your own praw.ini config file to use this command\n",
    "reddit = praw.Reddit(\"cs4243\")\n",
    "req_header = { \"User-Agent\": \"CS4243 crawler bot\", \"From\": \"insert email here\" }\n",
    "with open(SUBREDDITS_LIST_PATH, \"r\") as f:\n",
    "    sr_list = [ x.strip() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get image metadata first before deciding on images to download. Due to the long tailed distribution, and for a more representative distribution of scores, we download 500 images per subreddit so that the calculated percentiles are representative, and that there are enough images in the popular class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: THIS CODE SEGMENT CAN TAKE MORE THAN TEN MINUTES TO RUN!\n",
    "now = datetime.datetime.now()\n",
    "unixnow = int(datetime.datetime.timestamp(now))\n",
    "PRELIMINARY_PATH = f\"./crawl_{now.month:02}{now.day:02}.csv\"\n",
    "\n",
    "with open(PRELIMINARY_PATH, 'w') as datafile:\n",
    "    datafile.write(\"ID,SCORE,SUBREDDIT,URL,UNIX TIME,UPVOTE RATIO\\n\")\n",
    "    for sr in sr_list:\n",
    "        count = 0\n",
    "        for submission in reddit.subreddit(sr).new(limit=None):\n",
    "            # posts are at least one week old, for score stability\n",
    "            if (unixnow - submission.created_utc) > 604800:  \n",
    "                srname = submission.subreddit.display_name.lower()\n",
    "                if submission.url[-3:] not in ACCEPTABLE_EXTENSIONS:\n",
    "                    continue\n",
    "                datafile.write(f\"{submission.id},{submission.score},{srname},{submission.url},\" + \\\n",
    "                                    f\"{submission.created_utc},{submission.upvote_ratio}\\n\")\n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    datafile.flush()\n",
    "                    print(f\"Sourcing images in {sr}: {count}/{MAX_IMG}\")\n",
    "                if count == MAX_IMG:\n",
    "                    datafile.flush()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing to download selected images based on class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for selecting images to download based on metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRELIMINARY_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m CSV_PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/reddit/processed_data.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000008vscode-remote?line=9'>10</a>\u001b[0m MAX_WORKERS \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(PRELIMINARY_PATH, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRELIMINARY_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import queue\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "PERCENTILE_BINS = [0.5, 0.9, 1.0]\n",
    "NUM_ROWS_PER_SUB_PER_PERCENT = 50\n",
    "CSV_PATH = \"./data/reddit/processed_data.csv\"\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "with open(PRELIMINARY_PATH, \"r\") as f:\n",
    "    data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort reddit post scores into classes based on percentile bins. After which, select an equal number of images per subreddit per bin such that the proportion of classes are roughly equal. This solves the problem of learning on long-tailed distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PERCENTILE'] = data['SCORE'].rank(pct=True)\n",
    "data['PERCENTILE BIN'] = np.digitize(\n",
    "    data['PERCENTILE'], PERCENTILE_BINS, right=True)\n",
    "data['PERCENTILE BIN'] = data['PERCENTILE BIN'].map(\n",
    "    {index: bin for index, bin in enumerate(PERCENTILE_BINS)})\n",
    "\n",
    "filtered_rows = []\n",
    "for sr in sr_list:\n",
    "    os.makedirs(f\"./data/reddit/{sr.lower()}\", exist_ok=True)\n",
    "    for percent in PERCENTILE_BINS:\n",
    "        subdata = data[(data[\"SUBREDDIT\"] == sr.lower()) & (data[\"PERCENTILE BIN\"] == percent)]\n",
    "        filtered_rows.extend(subdata.head(NUM_ROWS_PER_SUB_PER_PERCENT).values.tolist())\n",
    "output = pd.DataFrame(filtered_rows, columns=data.columns)\n",
    "with open(CSV_PATH, \"w\") as f:\n",
    "    output.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some threading tools to help make downloading images faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_pool = queue.Queue()\n",
    "def get_image(url, filename) :\n",
    "    req = requests.get(url, stream=True, headers=req_header)\n",
    "    if not req.ok:\n",
    "        return\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in req.iter_content(1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    with Image.open(filename) as im:\n",
    "        im.thumbnail(MAX_RESOLUTION)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(filename[:-3]+\"jpeg\", \"JPEG\", quality=50, optimize=True)\n",
    "    os.remove(filename)\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        try:\n",
    "            subr, id, url = job_pool.get(timeout=60)\n",
    "            get_image(url, f\"./data/reddit/{subr.lower()}/{id}.{url[-3:]}\")\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        job_pool.task_done()\n",
    "\n",
    "all_threads = [threading.Thread(target=worker) for _ in range(MAX_WORKERS)]\n",
    "for t in all_threads:\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed with the downloading of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: THIS CODE SEGMENT CAN TAKE MORE THAN TEN MINUTES TO RUN!\n",
    "for srname, id, url in zip(data[\"SUBREDDIT\"], data[\"ID\"], data[\"URL\"]):\n",
    "    print(f\"{srname} {id} {url}\")\n",
    "    job_pool.put((srname, id, url))\n",
    "job_pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate Reddit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the images, collate all image metadata again to verify dataset. The csv file for image metadata and json files will be used in the model notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000016vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000016vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000016vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000016vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.27.66.69/home/nigelnnk/Documents/cs4243/image-popularity/crawler/reddit_crawler.ipynb#ch0000016vscode-remote?line=10'>11</a>\u001b[0m PERCENTILE_BINS \u001b[39m=\u001b[39m [\u001b[39m0.5\u001b[39m, \u001b[39m0.9\u001b[39m, \u001b[39m1.0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PERCENTILE_BINS = [0.5, 0.9, 1.0]\n",
    "\n",
    "def split_dataset(\n",
    "        dataset,\n",
    "        train_size,\n",
    "        val_size,\n",
    "        stratify_by=None,\n",
    "        random_seed=0):\n",
    "    if stratify_by:\n",
    "        stratify = dataset[stratify_by]\n",
    "    else:\n",
    "        stratify = None\n",
    "    train, val_test = train_test_split(\n",
    "        dataset,\n",
    "        train_size=train_size,\n",
    "        random_state=random_seed,\n",
    "        shuffle=True,\n",
    "        stratify=stratify)\n",
    "\n",
    "    if stratify_by:\n",
    "        stratify = val_test[stratify_by]\n",
    "    else:\n",
    "        stratify = None\n",
    "    val, test = train_test_split(\n",
    "        val_test,\n",
    "        train_size=val_size / (1.0 - train_size),\n",
    "        random_state=random_seed,\n",
    "        shuffle=True,\n",
    "        stratify=stratify)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def collate_reddit_data(\n",
    "        data_path,\n",
    "        reddit_levels_path,\n",
    "        output_path,\n",
    "        labels_path,\n",
    "        min_posts=1,\n",
    "        train_size=0.8,\n",
    "        val_size=0.1):\n",
    "    data_path = Path(data_path)\n",
    "    print(f'Reading data from: {data_path}')\n",
    "\n",
    "    csv_paths = list(data_path.glob('**/*.csv'))\n",
    "    csv_paths.sort()\n",
    "\n",
    "    image_paths = data_path.glob('**/*.jpeg')\n",
    "    image_id_to_path = {path.stem: path for path in image_paths}\n",
    "\n",
    "    # Load individual subreddit data\n",
    "    skipped_subreddits = []\n",
    "    data_list = []\n",
    "    for csv_path in tqdm(csv_paths):\n",
    "        data = pd.read_csv(csv_path, skiprows=2, on_bad_lines='skip')\n",
    "        data = data[data['ID'].isin(image_id_to_path)]\n",
    "        if len(data) < min_posts:\n",
    "            skipped_subreddits.append(data['SUBREDDIT'][0])\n",
    "            continue\n",
    "\n",
    "        data['PATH'] = data['ID'].map(image_id_to_path)\n",
    "\n",
    "        # Get percentile and percentile bin for each post in subreddit\n",
    "        data['PERCENTILE'] = data['SCORE'].rank(pct=True)\n",
    "        data['PERCENTILE BIN'] = np.digitize(\n",
    "            data['PERCENTILE'], PERCENTILE_BINS, right=True)\n",
    "        data['PERCENTILE BIN'] = data['PERCENTILE BIN'].map(\n",
    "            {index: bin for index, bin in enumerate(PERCENTILE_BINS)})\n",
    "\n",
    "        data_list.append(data)\n",
    "    data = pd.concat(data_list, ignore_index=True)\n",
    "    print(f'Skipped subreddits: {skipped_subreddits}')\n",
    "\n",
    "    # Merge reddit levels\n",
    "    reddit_levels = pd.read_csv(reddit_levels_path)\n",
    "    data = pd.merge(data, reddit_levels, how='left', on='SUBREDDIT')\n",
    "\n",
    "    # Create and save labels\n",
    "    labels = {\n",
    "        'percentile_bin': PERCENTILE_BINS,\n",
    "    }\n",
    "    for level in reddit_levels:\n",
    "        labels[level.lower()] = list(data[level].dropna().unique())\n",
    "    with open(labels_path, 'w') as file:\n",
    "        json.dump(labels, file, indent=4)\n",
    "    print(f'Saved labels to {labels_path}')\n",
    "\n",
    "    # Split dataset\n",
    "    train, val, test = split_dataset(\n",
    "        data,\n",
    "        train_size,\n",
    "        val_size,\n",
    "        stratify_by='PERCENTILE BIN',\n",
    "        random_seed=0)\n",
    "    train['SPLIT'] = 'train'\n",
    "    val['SPLIT'] = 'val'\n",
    "    test['SPLIT'] = 'test'\n",
    "    data = pd.concat([train, val, test], ignore_index=True)\n",
    "\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f'Saved data to {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_path': 'data/reddit',\n",
    "    'labels_path': 'data/reddit_labels.json',\n",
    "    'reddit_levels_path': 'dataset/reddit_levels.csv',\n",
    "    'output_path': 'data/reddit_data.csv',\n",
    "    'min_posts': 500,\n",
    "    'train_size': 0.8,\n",
    "    'val_size': 0.1\n",
    "}\n",
    "collate_reddit_data(**CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multireddit,0.5,0.9,1.0,\n",
      "imraces,400,400,388,0.058\n",
      "imcharacters,600,600,588,0.087\n",
      "imarchitecture,200,200,189,0.029\n",
      "imtechnology,500,500,478,0.072\n",
      "imlandscapes,600,600,587,0.087\n",
      "immonsters,550,550,530,0.079\n",
      "sfwpornnature,800,800,776,0.116\n",
      "sfwpornsynthetic,1300,1300,1259,0.188\n",
      "sfwpornaesthetic,1050,1050,1029,0.153\n",
      "sfwpornorganic,400,400,396,0.058\n",
      "sfwpornscholastic,250,250,246,0.036\n",
      ",0.336,0.336,0.327,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(Path('../data/short_reddit_data.csv'), on_bad_lines='skip')\n",
    "labels_path = \"../data/reddit_labels.json\"\n",
    "with open(labels_path, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "sr_list = labels[\"subreddit\"]\n",
    "mr_list = labels[\"multireddit\"]\n",
    "percentile = labels[\"percentile_bin\"]\n",
    "ans = f\"multireddit,{','.join([str(x) for x in percentile])},\"\n",
    "total_size = len(data)\n",
    "for mr in mr_list:\n",
    "    line = f\"\\n{mr},\"\n",
    "    count = 0\n",
    "    for pc in percentile:\n",
    "        subdata = data[(data[\"MULTIREDDIT\"] == mr.lower()) & (data[\"PERCENTILE BIN\"] == pc)]\n",
    "        line += f\"{len(subdata)},\"\n",
    "        count += len(subdata)\n",
    "    ans += line + f\"{count/total_size:.03f}\"\n",
    "line = \"\\n,\"\n",
    "for pc in percentile:\n",
    "    subdata = data[data[\"PERCENTILE BIN\"] == pc]\n",
    "    line += f\"{len(subdata)/total_size:.03f},\"\n",
    "ans += line\n",
    "\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5df0192bec759d1f7484f54f673824e28388fbedcfd4d53bba5ff9ed1739130e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('reddit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
